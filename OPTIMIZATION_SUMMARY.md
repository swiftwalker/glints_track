# è®­ç»ƒç³»ç»Ÿä¼˜åŒ–å®Œæˆæ€»ç»“

## âœ… å·²å®Œæˆçš„åŠŸèƒ½

### 1. **å®Œæ•´çš„ Checkpoint ç³»ç»Ÿ**
- âœ… ä¿å­˜æ¨¡å‹æƒé‡ + ä¼˜åŒ–å™¨çŠ¶æ€ + è®­ç»ƒå‚æ•°
- âœ… æ”¯æŒæ¢å¤è®­ç»ƒï¼ˆä»ä»»æ„ checkpoint ç»§ç»­ï¼‰
- âœ… è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
- âœ… å®šæœŸä¿å­˜ checkpointï¼ˆå¯é…ç½®é¢‘ç‡ï¼‰
- âœ… å§‹ç»ˆä¿å­˜æœ€æ–° checkpoint

### 2. **ç»“æ„åŒ–è¾“å‡ºç›®å½•**
```
runs/train_YYYYMMDD_HHMMSS/
â”œâ”€â”€ training.log              # æ–‡æœ¬æ—¥å¿—
â”œâ”€â”€ metrics.json              # JSON æ ¼å¼æŒ‡æ ‡
â”œâ”€â”€ training_args.json        # è®­ç»ƒå‚æ•°
â”œâ”€â”€ best_model.pth            # æœ€ä½³æ¨¡å‹ï¼ˆå®Œæ•´ï¼‰
â”œâ”€â”€ best_model_weights.pt     # æœ€ä½³æ¨¡å‹æƒé‡ï¼ˆå…¼å®¹ï¼‰
â”œâ”€â”€ latest_checkpoint.pth     # æœ€æ–° checkpoint
â””â”€â”€ checkpoints/
    â”œâ”€â”€ checkpoint_epoch_010.pth
    â”œâ”€â”€ checkpoint_epoch_020.pth
    â””â”€â”€ ...
```

### 3. **è®­ç»ƒæ—¥å¿—ç³»ç»Ÿ**
- âœ… æ–‡æœ¬æ ¼å¼æ—¥å¿—ï¼ˆæ—  tqdm è¿›åº¦æ¡ï¼‰
- âœ… åŒ…å«æ—¶é—´æˆ³çš„è¯¦ç»†è®°å½•
- âœ… è®­ç»ƒ/éªŒè¯æŒ‡æ ‡è®°å½•
- âœ… JSON æ ¼å¼çš„ç»“æ„åŒ–æŒ‡æ ‡

### 4. **æ•°æ®åŠ è½½ä¼˜åŒ–**ï¼ˆå·²æœ‰ï¼‰
- âœ… å¤šè¿›ç¨‹æ•°æ®åŠ è½½ï¼ˆnum_workersï¼‰
- âœ… æ•°æ®é¢„åŠ è½½åˆ°å†…å­˜
- âœ… å…±äº«å†…å­˜æœºåˆ¶ï¼ˆå¤šè¿›ç¨‹å¤ç”¨ï¼‰
- âœ… pin_memory åŠ é€Ÿ GPU ä¼ è¾“

## ğŸ¯ æ ¸å¿ƒç±»å’ŒåŠŸèƒ½

### `TrainingLogger`
è´Ÿè´£è®­ç»ƒæ—¥å¿—è®°å½•ï¼š
- `log()`: è®°å½•æ—¥å¿—æ¶ˆæ¯ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
- `log_epoch()`: è®°å½• epoch æŒ‡æ ‡
- è‡ªåŠ¨ä¿å­˜åˆ° `training.log` å’Œ `metrics.json`

### `CheckpointManager`
è´Ÿè´£ checkpoint ç®¡ç†ï¼š
- `save_checkpoint()`: ä¿å­˜å®Œæ•´ checkpoint
- `load_checkpoint()`: åŠ è½½ checkpoint å¹¶æ¢å¤çŠ¶æ€
- `save_latest()`: ä¿å­˜æœ€æ–° checkpointï¼ˆè¦†ç›–å¼ï¼‰
- `save_training_args()`: ä¿å­˜è®­ç»ƒå‚æ•°åˆ° JSON

### `SharedMemoryDatasetCache`ï¼ˆå·²æœ‰ï¼‰
è´Ÿè´£å…±äº«å†…å­˜ç®¡ç†ï¼š
- ä½¿ç”¨ MD5 å”¯ä¸€æ ‡è¯†æ•°æ®é›†
- è‡ªåŠ¨æ£€æµ‹å’Œè¿æ¥å·²å­˜åœ¨çš„å…±äº«å†…å­˜
- æ”¯æŒå¤šè¿›ç¨‹æ•°æ®å…±äº«

## ğŸ“ æ–°å¢å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--output_dir` | str | auto | è¾“å‡ºç›®å½• |
| `--resume` | str | None | æ¢å¤è®­ç»ƒçš„ checkpoint |
| `--save_freq` | int | 10 | Checkpoint ä¿å­˜é¢‘ç‡ |
| `--keep_last_n` | int | 3 | ä¿ç•™æœ€è¿‘ N ä¸ª checkpoint |
| `--gpu` | str | None | æŒ‡å®š GPU è®¾å¤‡ï¼ˆå¦‚ '0', '1', '0,1'ï¼‰|
| `--no_save_optimizer` | flag | False | ä¸ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå‡å°æ–‡ä»¶çº¦ 50%ï¼‰|

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€è®­ç»ƒ
```bash
python train_glint_unet.py --h5 train_256.h5 --epochs 50
```

### æŒ‡å®šè¾“å‡ºç›®å½•
```bash
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/focal_exp1 \
    --epochs 50
```

### æ¢å¤è®­ç»ƒ
```bash
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/focal_exp1 \
    --resume experiments/focal_exp1/latest_checkpoint.pth \
    --epochs 100
```

### æŒ‡å®š GPU è®­ç»ƒ
```bash
# ä½¿ç”¨ GPU 0
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/gpu0_exp \
    --gpu 0 \
    --epochs 50

# åœ¨ä¸åŒ GPU ä¸Šå¹¶è¡Œè¿è¡Œå¤šä¸ªå®éªŒ
python train_glint_unet.py --h5 train_256.h5 --output_dir exp1 --gpu 0 --epochs 50 &
python train_glint_unet.py --h5 train_256.h5 --output_dir exp2 --gpu 1 --epochs 50 &
wait
```

### å¤šå®éªŒå¹¶è¡Œï¼ˆå…±äº«å†…å­˜ï¼‰
```bash
# å®éªŒ 1
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/focal \
    --shared_memory \
    --loss focal &

# å®éªŒ 2
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/hybrid \
    --shared_memory \
    --loss hybrid &

wait
```

## ğŸ“Š Checkpoint å†…å®¹

æ¯ä¸ª checkpoint åŒ…å«ï¼š
```python
{
    'epoch': 42,
    'model_state_dict': {...},      # æ¨¡å‹æƒé‡
    'optimizer_state_dict': {...},  # ä¼˜åŒ–å™¨çŠ¶æ€
    'train_args': {...},            # è®­ç»ƒå‚æ•°
    'metrics': {                    # å½“å‰æŒ‡æ ‡
        'train': {...},
        'val': {...}
    },
    'timestamp': '2025-11-06 14:30:22'
}
```

## ğŸ”„ æ¢å¤è®­ç»ƒæµç¨‹

1. åŠ è½½ checkpoint
2. æ¢å¤æ¨¡å‹æƒé‡
3. æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå­¦ä¹ ç‡ã€åŠ¨é‡ç­‰ï¼‰
4. ä»ä¸‹ä¸€ä¸ª epoch ç»§ç»­è®­ç»ƒ
5. ä¿æŒä¹‹å‰çš„æœ€ä½³ loss è®°å½•

## ğŸ“ˆ æ—¥å¿—ç¤ºä¾‹

### training.log
```
[2025-11-06 14:30:22] ================================================================================
[2025-11-06 14:30:22] å¼€å§‹è®­ç»ƒ
[2025-11-06 14:30:22] è¾“å‡ºç›®å½•: runs/train_20251106_143022
[2025-11-06 14:30:22] æ•°æ®é›†: train_256.h5
[2025-11-06 14:30:25] è®­ç»ƒé›†æ ·æœ¬æ•°: 900, éªŒè¯é›†æ ·æœ¬æ•°: 100
[2025-11-06 14:31:10] 
Epoch 001 (è€—æ—¶: 45.32s, LR: 1.00e-03):
  Train: total=0.2345 focal=0.1234 bce=0.0000 dice=0.0000 div=0.0111
  Val:   total=0.1987 focal=0.1045 bce=0.0000 dice=0.0000 div=0.0092
[2025-11-06 14:31:10]   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: 0.1987)
```

### metrics.json
```json
[
  {
    "epoch": 1,
    "train": {"total": 0.2345, "focal": 0.1234, ...},
    "val": {"total": 0.1987, "focal": 0.1045, ...},
    "lr": 0.001,
    "epoch_time": 45.32
  },
  ...
]
```

## ğŸ”§ æŠ€æœ¯å®ç°

### æ—¥å¿—è®°å½•
- ä½¿ç”¨ `datetime` ç”Ÿæˆæ—¶é—´æˆ³
- åŒæ—¶å†™å…¥æ–‡ä»¶å’Œæ§åˆ¶å°
- tqdm è¿›åº¦æ¡å¯é€‰ç¦ç”¨ï¼ˆé€šè¿‡ logger å‚æ•°ï¼‰

### Checkpoint ä¿å­˜
- PyTorch åŸç”Ÿ `torch.save()`
- ä¿å­˜å®Œæ•´çŠ¶æ€å­—å…¸
- è‡ªåŠ¨åˆ›å»ºå­ç›®å½•

### å‚æ•°åºåˆ—åŒ–
- ä½¿ç”¨ JSON æ ¼å¼
- è‡ªåŠ¨è¿‡æ»¤ä¸å¯åºåˆ—åŒ–å¯¹è±¡
- ä¿ç•™æ‰€æœ‰è®­ç»ƒé…ç½®

## ğŸ’¡ æœ€ä½³å®è·µ

1. **ä½¿ç”¨æœ‰æ„ä¹‰çš„è¾“å‡ºç›®å½•å**
   ```bash
   --output_dir experiments/focal_lr1e3_batch16
   ```

2. **å®šæœŸæ£€æŸ¥æ—¥å¿—**
   ```bash
   tail -f runs/xxx/training.log
   ```

3. **å¤‡ä»½é‡è¦å®éªŒ**
   ```bash
   cp -r runs/focal_exp1 backups/
   ```

4. **ä½¿ç”¨å…±äº«å†…å­˜è¿›è¡Œå¤šå®éªŒ**
   ```bash
   --shared_memory  # å¤šè¿›ç¨‹åœºæ™¯
   ```

5. **ä»æœ€æ–° checkpoint æ¢å¤**
   ```bash
   --resume runs/xxx/latest_checkpoint.pth
   ```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- `TRAINING_GUIDE.md`: å®Œæ•´ä½¿ç”¨æŒ‡å—
- `SHARED_MEMORY_USAGE.md`: å…±äº«å†…å­˜è¯¦ç»†æ–‡æ¡£
- `SHARED_MEMORY_QUICKSTART.md`: å…±äº«å†…å­˜å¿«é€Ÿå¼€å§‹

## ğŸ§ª æµ‹è¯•

è¿è¡Œæµ‹è¯•éªŒè¯åŠŸèƒ½ï¼š
```bash
python test_training_system.py
```

## ğŸ”— å‘åå…¼å®¹

æ—§å‚æ•°ä»ç„¶æ”¯æŒï¼ˆä½†å·²å¼ƒç”¨ï¼‰ï¼š
- `--model_path` â†’ ä½¿ç”¨ `--output_dir`
- `--save` â†’ ä½¿ç”¨ `--output_dir`

æ—§æ¨¡å‹åŠ è½½æ–¹å¼ä»ç„¶å¯ç”¨ï¼š
```python
# ä»…åŠ è½½æƒé‡
model.load_state_dict(torch.load('best_model_weights.pt'))
```

## ğŸ‰ æ€§èƒ½æå‡æ€»ç»“

### å®Œæ•´ä¼˜åŒ–é“¾æ¡ï¼š
1. **æ•°æ®åŠ è½½**: num_workers + pin_memory
2. **å†…å­˜ä¼˜åŒ–**: preload + shared_memory
3. **è®­ç»ƒç®¡ç†**: checkpoint + logging
4. **å®éªŒè¿½è¸ª**: ç»“æ„åŒ–è¾“å‡º

### é¢„æœŸæ•ˆæœï¼š
- âœ… GPU åˆ©ç”¨ç‡: 30-50% â†’ 80-95%
- âœ… è®­ç»ƒé€Ÿåº¦: æå‡ 2-4 å€
- âœ… å†…å­˜ä½¿ç”¨: å¤šè¿›ç¨‹èŠ‚çœ 50-75%
- âœ… å®éªŒç®¡ç†: å®Œå…¨å¯è¿½æº¯å’Œå¯æ¢å¤
