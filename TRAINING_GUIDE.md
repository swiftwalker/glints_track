# å®Œæ•´è®­ç»ƒç³»ç»Ÿä½¿ç”¨æŒ‡å—

## ğŸ¯ æ–°çš„ Checkpoint ç³»ç»Ÿ

è®­ç»ƒç³»ç»Ÿå·²å®Œå…¨é‡æ„ï¼Œç°åœ¨æ”¯æŒï¼š
- âœ… å®Œæ•´çš„ checkpoint ä¿å­˜ï¼ˆæ¨¡å‹ + ä¼˜åŒ–å™¨ + è®­ç»ƒå‚æ•°ï¼‰
- âœ… æ¢å¤è®­ç»ƒåŠŸèƒ½
- âœ… ç»“æ„åŒ–è¾“å‡ºç›®å½•
- âœ… è¯¦ç»†çš„è®­ç»ƒæ—¥å¿—ï¼ˆæ—  tqdm è¿›åº¦ä¿¡æ¯ï¼‰
- âœ… è®­ç»ƒæŒ‡æ ‡çš„ JSON è®°å½•

## ğŸ“ è¾“å‡ºç›®å½•ç»“æ„

```
runs/
â””â”€â”€ train_20251106_143022/          # è®­ç»ƒè¾“å‡ºç›®å½•
    â”œâ”€â”€ training.log                # æ–‡æœ¬æ—¥å¿—ï¼ˆå®Œæ•´è®­ç»ƒè®°å½•ï¼‰
    â”œâ”€â”€ metrics.json                # è®­ç»ƒæŒ‡æ ‡ï¼ˆJSON æ ¼å¼ï¼‰
    â”œâ”€â”€ training_args.json          # è®­ç»ƒå‚æ•°é…ç½®
    â”œâ”€â”€ best_model.pth              # æœ€ä½³æ¨¡å‹ï¼ˆå®Œæ•´ checkpointï¼‰
    â”œâ”€â”€ best_model_weights.pt       # æœ€ä½³æ¨¡å‹æƒé‡ï¼ˆä»…æƒé‡ï¼Œå…¼å®¹æ—§ä»£ç ï¼‰
    â”œâ”€â”€ latest_checkpoint.pth       # æœ€æ–° checkpointï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
    â””â”€â”€ checkpoints/                # å®šæœŸä¿å­˜çš„ checkpoint
        â”œâ”€â”€ checkpoint_epoch_010.pth
        â”œâ”€â”€ checkpoint_epoch_020.pth
        â””â”€â”€ checkpoint_epoch_030.pth
```

## ğŸš€ åŸºç¡€ä½¿ç”¨

### 1. æ–°å»ºè®­ç»ƒï¼ˆè‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•ï¼‰

```bash
python train_glint_unet.py \
    --h5 train_256.h5 \
    --epochs 50 \
    --batch 16 \
    --lr 1e-3
```

è¾“å‡ºç›®å½•å°†è‡ªåŠ¨åˆ›å»ºä¸º `runs/train_YYYYMMDD_HHMMSS/`

### 2. æŒ‡å®šè¾“å‡ºç›®å½•

```bash
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir runs/focal_experiment \
    --epochs 50 \
    --loss focal
```

### 3. æ¢å¤è®­ç»ƒ

```bash
# æ–¹æ³• 1: ä» latest_checkpoint æ¢å¤
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir runs/focal_experiment \
    --resume runs/focal_experiment/latest_checkpoint.pth \
    --epochs 100

# æ–¹æ³• 2: ä»ç‰¹å®š checkpoint æ¢å¤
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir runs/focal_experiment \
    --resume runs/focal_experiment/checkpoints/checkpoint_epoch_020.pth \
    --epochs 100
```

## ğŸ“Š æ—¥å¿—å’ŒæŒ‡æ ‡

### training.logï¼ˆæ–‡æœ¬æ—¥å¿—ï¼‰

```
[2025-11-06 14:30:22] è®­ç»ƒæ—¥å¿— - å¼€å§‹æ—¶é—´: 2025-11-06 14:30:22
[2025-11-06 14:30:22] ================================================================================
[2025-11-06 14:30:22] å¼€å§‹è®­ç»ƒ
[2025-11-06 14:30:22] è¾“å‡ºç›®å½•: runs/train_20251106_143022
[2025-11-06 14:30:22] æ•°æ®é›†: train_256.h5
...
[2025-11-06 14:35:15] Epoch 001 (è€—æ—¶: 45.32s, LR: 1.00e-03):
[2025-11-06 14:35:15]   Train: total=0.2345 focal=0.1234 bce=0.0000 dice=0.0000 div=0.0111
[2025-11-06 14:35:15]   Val:   total=0.1987 focal=0.1045 bce=0.0000 dice=0.0000 div=0.0092
[2025-11-06 14:35:15]   âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: 0.1987)
```

### metrics.jsonï¼ˆç»“æ„åŒ–æŒ‡æ ‡ï¼‰

```json
[
  {
    "epoch": 1,
    "train": {
      "total": 0.2345,
      "focal": 0.1234,
      "bce": 0.0,
      "dice": 0.0,
      "div": 0.0111
    },
    "val": {
      "total": 0.1987,
      "focal": 0.1045,
      "bce": 0.0,
      "dice": 0.0,
      "div": 0.0092
    },
    "lr": 0.001,
    "epoch_time": 45.32
  }
]
```

## âš™ï¸ æ–°å¢å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--output_dir` | `runs/train_YYYYMMDD_HHMMSS` | è¾“å‡ºç›®å½• |
| `--resume` | None | æ¢å¤è®­ç»ƒçš„ checkpoint è·¯å¾„ |
| `--save_freq` | 10 | æ¯ N ä¸ª epoch ä¿å­˜ä¸€æ¬¡ checkpoint |
| `--keep_last_n` | 3 | ä¿ç•™æœ€è¿‘ N ä¸ª checkpointï¼ˆæœªå®ç°ï¼‰ |

## ğŸ“ å®Œæ•´å‚æ•°åˆ—è¡¨

### æ•°æ®å’Œè¾“å‡º
- `--h5`: HDF5 æ•°æ®é›†è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `--output_dir`: è¾“å‡ºç›®å½•
- `--resume`: æ¢å¤è®­ç»ƒçš„ checkpoint è·¯å¾„

### è®­ç»ƒå‚æ•°
- `--epochs`: è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ 50ï¼‰
- `--batch`: Batch sizeï¼ˆé»˜è®¤ 8ï¼‰
- `--lr`: å­¦ä¹ ç‡ï¼ˆé»˜è®¤ 1e-3ï¼‰
- `--val_split`: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.1ï¼‰
- `--gpu`: æŒ‡å®š GPU è®¾å¤‡ï¼ˆå¦‚ '0', '1', '0,1' ç­‰ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨é€‰æ‹©ï¼‰

### æ•°æ®åŠ è½½ä¼˜åŒ–
- `--num_workers`: DataLoader å·¥ä½œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤ 4ï¼‰
- `--preload`: é¢„åŠ è½½æ•°æ®åˆ°å†…å­˜ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
- `--no_preload`: ç¦ç”¨é¢„åŠ è½½
- `--shared_memory`: ä½¿ç”¨å…±äº«å†…å­˜

### Checkpoint ç›¸å…³
- `--save_freq`: ä¿å­˜é¢‘ç‡ï¼ˆé»˜è®¤æ¯ 10 epochï¼‰
- `--no_save_optimizer`: ä¸ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå‡å°æ–‡ä»¶å¤§å°çº¦ 50%ï¼Œä½†æ— æ³•å®Œç¾æ¢å¤è®­ç»ƒï¼‰
  - é»˜è®¤ï¼šä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆæ¨èï¼Œæ”¯æŒå®Œç¾æ¢å¤è®­ç»ƒï¼‰

### Loss å‡½æ•°
- `--loss`: Loss ç±»å‹ï¼ˆfocal/bce/dice/hybridï¼‰
- `--alpha`, `--gamma`: Focal Loss å‚æ•°
- `--lam_focal`, `--lam_bce`, `--lam_dice`: æŸå¤±æƒé‡
- `--div_weight`, `--div_mode`: ç›¸ä¼¼åº¦æƒ©ç½š
- `--lam_agg`, `--agg_mode`: èšåˆå‚æ•°

## ğŸ¬ å®é™…ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: å¿«é€Ÿå®éªŒ

```bash
# å¿«é€Ÿæµ‹è¯•ï¼Œè‡ªåŠ¨ä¿å­˜åˆ° runs/
python train_glint_unet.py --h5 train_256.h5 --epochs 10 --batch 32
```

### åœºæ™¯ 2: æ­£å¼è®­ç»ƒ

```bash
# æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä¾¿äºç®¡ç†
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/focal_baseline \
    --epochs 100 \
    --batch 16 \
    --lr 1e-3 \
    --loss focal \
    --save_freq 5
```

### åœºæ™¯ 3: è®­ç»ƒä¸­æ–­åæ¢å¤

```bash
# ä»ä¸­æ–­å¤„ç»§ç»­è®­ç»ƒ
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/focal_baseline \
    --resume experiments/focal_baseline/latest_checkpoint.pth \
    --epochs 200
```

### åœºæ™¯ 4: æŒ‡å®š GPU è®­ç»ƒ

```bash
# ä½¿ç”¨ GPU 0
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/gpu0_exp \
    --gpu 0 \
    --epochs 50

# ä½¿ç”¨ GPU 1
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/gpu1_exp \
    --gpu 1 \
    --epochs 50

# åŒæ—¶åœ¨å¤šä¸ª GPU ä¸Šè¿è¡Œä¸åŒå®éªŒ
python train_glint_unet.py --h5 train_256.h5 --output_dir exp1 --gpu 0 --epochs 50 &
python train_glint_unet.py --h5 train_256.h5 --output_dir exp2 --gpu 1 --epochs 50 &
wait
```

### åœºæ™¯ 5: å¤šå®éªŒå¯¹æ¯”

```bash
# å®éªŒ 1: Focal Loss
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/exp1_focal \
    --loss focal \
    --epochs 50 &

# å®éªŒ 2: Hybrid Loss
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/exp2_hybrid \
    --loss hybrid \
    --epochs 50 &

# å®éªŒ 3: BCE Loss
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/exp3_bce \
    --loss bce \
    --epochs 50 &

wait
```

### åœºæ™¯ 6: ä¸ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå‡å°æ–‡ä»¶å¤§å°ï¼‰

```bash
# é€‚åˆï¼šåªéœ€è¦æ¨¡å‹æƒé‡ï¼Œä¸éœ€è¦æ¢å¤è®­ç»ƒçš„åœºæ™¯
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/inference_only \
    --no_save_optimizer \
    --epochs 100

# æ–‡ä»¶å¤§å°å¯¹æ¯”ï¼ˆç¤ºä¾‹ï¼‰ï¼š
# ä¿å­˜ä¼˜åŒ–å™¨:     checkpoint.pth ~200MB
# ä¸ä¿å­˜ä¼˜åŒ–å™¨:   checkpoint.pth ~100MB (èŠ‚çœçº¦50%)
```

### åœºæ™¯ 7: å­¦ä¹ ç‡è°ƒä¼˜

```bash
for lr in 1e-2 5e-3 1e-3 5e-4 1e-4; do
    python train_glint_unet.py \
        --h5 train_256.h5 \
        --output_dir experiments/lr_tuning/lr_${lr} \
        --lr ${lr} \
        --epochs 30 \
        --shared_memory &
done
wait
```

## ğŸ” æŸ¥çœ‹è®­ç»ƒç»“æœ

### æŸ¥çœ‹æ—¥å¿—

```bash
# å®æ—¶æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f runs/train_20251106_143022/training.log

# æŸ¥çœ‹å®Œæ•´æ—¥å¿—
cat runs/train_20251106_143022/training.log

# æœç´¢æœ€ä½³æ€§èƒ½
grep "ä¿å­˜æœ€ä½³æ¨¡å‹" runs/train_20251106_143022/training.log
```

### åˆ†ææŒ‡æ ‡

```python
import json
import matplotlib.pyplot as plt

# è¯»å–æŒ‡æ ‡
with open('runs/train_20251106_143022/metrics.json', 'r') as f:
    metrics = json.load(f)

# ç»˜åˆ¶ loss æ›²çº¿
epochs = [m['epoch'] for m in metrics]
train_loss = [m['train']['total'] for m in metrics]
val_loss = [m['val']['total'] for m in metrics]

plt.plot(epochs, train_loss, label='Train')
plt.plot(epochs, val_loss, label='Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

## ğŸ”„ ä¸æ—§ç‰ˆæœ¬çš„å…¼å®¹æ€§

æ—§å‚æ•°ä»ç„¶æ”¯æŒï¼ˆå·²å¼ƒç”¨ï¼‰ï¼š
- `--model_path`: ä½¿ç”¨ `--output_dir` æ›¿ä»£
- `--save`: ä½¿ç”¨ `--output_dir` æ›¿ä»£

æ—§æ¨¡å‹åŠ è½½æ–¹å¼ï¼š
```python
# æ–¹æ³• 1: ä»…åŠ è½½æƒé‡ï¼ˆå…¼å®¹ï¼‰
model.load_state_dict(torch.load('runs/xxx/best_model_weights.pt'))

# æ–¹æ³• 2: åŠ è½½å®Œæ•´ checkpointï¼ˆæ¨èï¼‰
checkpoint = torch.load('runs/xxx/best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
```

## ğŸ’¡ æœ€ä½³å®è·µ

1. **å‘½åè¾“å‡ºç›®å½•**ï¼šä½¿ç”¨æœ‰æ„ä¹‰çš„åç§°
   ```bash
   --output_dir experiments/focal_lr1e3_batch16
   ```

2. **å®šæœŸä¿å­˜**ï¼šé‡è¦å®éªŒè®¾ç½®è¾ƒå°çš„ `--save_freq`
   ```bash
   --save_freq 5
   ```

3. **ä½¿ç”¨å…±äº«å†…å­˜**ï¼šå¤šå®éªŒå¹¶è¡Œæ—¶
   ```bash
   --shared_memory
   ```

4. **æ¢å¤è®­ç»ƒ**ï¼šæ„å¤–ä¸­æ–­åç»§ç»­
   ```bash
   --resume path/to/latest_checkpoint.pth
   ```

5. **æŸ¥çœ‹æ—¥å¿—**ï¼šå®šæœŸæ£€æŸ¥è®­ç»ƒè¿›åº¦
   ```bash
   tail -f runs/xxx/training.log
   ```

## ğŸ†˜ å¸¸è§é—®é¢˜

**Q: å¦‚ä½•ä» checkpoint æå–æ¨¡å‹æƒé‡ï¼Ÿ**
```python
checkpoint = torch.load('runs/xxx/best_model.pth')
torch.save(checkpoint['model_state_dict'], 'model_weights_only.pt')
```

**Q: å¦‚ä½•æ›´æ”¹æ¢å¤è®­ç»ƒçš„å­¦ä¹ ç‡ï¼Ÿ**
```bash
# checkpoint ä¼šæ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä½†å¯ä»¥æ‰‹åŠ¨ä¿®æ”¹
python train_glint_unet.py \
    --resume runs/xxx/latest_checkpoint.pth \
    --lr 1e-4  # æ–°çš„å­¦ä¹ ç‡ä¼šè¦†ç›–
```

**Q: è¾“å‡ºç›®å½•å·²å­˜åœ¨æ€ä¹ˆåŠï¼Ÿ**
- ç³»ç»Ÿä¼šç»§ç»­ä½¿ç”¨è¯¥ç›®å½•
- æ–°çš„æ—¥å¿—ä¼šè¿½åŠ åˆ°æ–‡ä»¶
- å»ºè®®ä½¿ç”¨ä¸åŒçš„è¾“å‡ºç›®å½•æˆ–æ¸…ç†æ—§æ–‡ä»¶

**Q: å¦‚ä½•æ¸…ç†æ—§çš„ checkpointï¼Ÿ**
```bash
# åªä¿ç•™æœ€ä½³æ¨¡å‹å’Œæœ€æ–° checkpoint
cd runs/xxx/checkpoints
ls -t | tail -n +4 | xargs rm  # ä¿ç•™æœ€æ–° 3 ä¸ª
```

**Q: ä¸ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ**
- âœ… ä¼˜ç‚¹ï¼šæ–‡ä»¶å¤§å°å‡å°‘çº¦ 50%
- âŒ ç¼ºç‚¹ï¼šæ— æ³•å®Œç¾æ¢å¤è®­ç»ƒï¼ˆå­¦ä¹ ç‡è°ƒåº¦ã€momentum ç­‰ä¼šä¸¢å¤±ï¼‰
- ğŸ’¡ å»ºè®®ï¼šå¦‚æœåªéœ€è¦æ¨¡å‹æ¨ç†ï¼Œä½¿ç”¨ `--no_save_optimizer`
- ğŸ’¡ å»ºè®®ï¼šå¦‚æœéœ€è¦æ¢å¤è®­ç»ƒï¼Œä¿æŒé»˜è®¤ï¼ˆä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰
