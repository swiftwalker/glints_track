import torch, h5py, random, os, argparse, hashlib, json, time
from datetime import datetime
import numpy as np
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from multiprocessing import shared_memory
from unet_glint import UNet
from losses import build_loss
from tqdm import tqdm

# ------------------------------
# Checkpoint å’Œæ—¥å¿—ç®¡ç†
# ------------------------------
class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨ï¼ˆä¸åŒ…å« tqdm è¿›åº¦æ¡ä¿¡æ¯ï¼‰"""
    def __init__(self, log_dir):
        self.log_dir = log_dir
        self.log_file = os.path.join(log_dir, "training.log")
        self.metrics_file = os.path.join(log_dir, "metrics.json")
        self.metrics_history = []
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´ 
        with open(self.log_file, 'w') as f:
            f.write(f"è®­ç»ƒæ—¥å¿— - å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")
    
    def log(self, message, print_console=True):
        """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}"
        
        # å†™å…¥æ–‡ä»¶
        with open(self.log_file, 'a') as f:
            f.write(log_message + "\n")
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        if print_console:
            print(log_message)
    
    def log_epoch(self, epoch, train_metrics, val_metrics, lr, epoch_time):
        """è®°å½• epoch æŒ‡æ ‡"""
        message = f"\nEpoch {epoch:03d} (è€—æ—¶: {epoch_time:.2f}s, LR: {lr:.2e}):\n"
        message += f"  Train: total={train_metrics['total']:.4f} focal={train_metrics['focal']:.4f} "
        message += f"bce={train_metrics['bce']:.4f} dice={train_metrics['dice']:.4f} div={train_metrics['div']:.4f}\n"
        message += f"  Val:   total={val_metrics['total']:.4f} focal={val_metrics['focal']:.4f} "
        message += f"bce={val_metrics['bce']:.4f} dice={val_metrics['dice']:.4f} div={val_metrics['div']:.4f}"
        
        self.log(message)
        
        # ä¿å­˜æŒ‡æ ‡åˆ° JSON
        self.metrics_history.append({
            'epoch': epoch,
            'train': train_metrics,
            'val': val_metrics,
            'lr': lr,
            'epoch_time': epoch_time
        })
        
        with open(self.metrics_file, 'w') as f:
            json.dump(self.metrics_history, f, indent=2)


class CheckpointManager:
    """Checkpoint ç®¡ç†å™¨"""
    def __init__(self, output_dir, save_optimizer=True):
        self.output_dir = output_dir
        self.checkpoints_dir = os.path.join(output_dir, "checkpoints")
        self.save_optimizer = save_optimizer
        os.makedirs(self.checkpoints_dir, exist_ok=True)
    
    def save_checkpoint(self, epoch, model, optimizer, train_args, metrics, 
                       is_best=False, filename=None):
        """
        ä¿å­˜å®Œæ•´çš„ checkpoint
        
        Args:
            epoch: å½“å‰ epoch
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            train_args: è®­ç»ƒå‚æ•°ï¼ˆargparse.Namespace æˆ– dictï¼‰
            metrics: å½“å‰æŒ‡æ ‡
            is_best: æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            filename: è‡ªå®šä¹‰æ–‡ä»¶å
        """
        # è½¬æ¢ train_args ä¸º dict
        if hasattr(train_args, '__dict__'):
            train_args_dict = vars(train_args)
        else:
            train_args_dict = train_args
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'train_args': train_args_dict,
            'metrics': metrics,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
        if self.save_optimizer and optimizer is not None:
            checkpoint['optimizer_state_dict'] = optimizer.state_dict()
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¿å­˜åˆ°æ ¹ç›®å½•
        if is_best:
            best_path = os.path.join(self.output_dir, "best_model.pth")
            torch.save(checkpoint, best_path)
            
            # åŒæ—¶ä¿å­˜çº¯æ¨¡å‹æƒé‡ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
            model_only_path = os.path.join(self.output_dir, "best_model_weights.pt")
            torch.save(model.state_dict(), model_only_path)
            
            return best_path
        else:
            # å®šæœŸä¿å­˜åˆ° checkpoints ç›®å½•
            if filename is None:
                filename = f"checkpoint_epoch_{epoch:03d}.pth"
            
            checkpoint_path = os.path.join(self.checkpoints_dir, filename)
            torch.save(checkpoint, checkpoint_path)
            
            return checkpoint_path
    
    def save_latest(self, epoch, model, optimizer, train_args, metrics):
        """ä¿å­˜ä¸º latest checkpointï¼ˆè¦†ç›–å¼ï¼‰"""
        latest_path = os.path.join(self.output_dir, "latest_checkpoint.pth")
        
        if hasattr(train_args, '__dict__'):
            train_args_dict = vars(train_args)
        else:
            train_args_dict = train_args
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'train_args': train_args_dict,
            'metrics': metrics,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
        if self.save_optimizer and optimizer is not None:
            checkpoint['optimizer_state_dict'] = optimizer.state_dict()
        
        torch.save(checkpoint, latest_path)
        return latest_path
    
    def load_checkpoint(self, checkpoint_path, model, optimizer=None, device='cuda'):
        """
        åŠ è½½ checkpoint
        
        Returns:
            dict: åŒ…å« epoch, train_args, metrics çš„å­—å…¸
        """
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint ä¸å­˜åœ¨: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        return {
            'epoch': checkpoint.get('epoch', 0),
            'train_args': checkpoint.get('train_args', {}),
            'metrics': checkpoint.get('metrics', {}),
            'timestamp': checkpoint.get('timestamp', 'unknown')
        }
    
    def get_latest_checkpoint(self):
        """è·å–æœ€æ–°çš„ checkpoint è·¯å¾„"""
        latest_path = os.path.join(self.output_dir, "latest_checkpoint.pth")
        if os.path.exists(latest_path):
            return latest_path
        return None
    
    def save_training_args(self, args):
        """ä¿å­˜è®­ç»ƒå‚æ•°åˆ° JSON"""
        args_path = os.path.join(self.output_dir, "training_args.json")
        
        if hasattr(args, '__dict__'):
            args_dict = vars(args)
        else:
            args_dict = args
        
        # è¿‡æ»¤æ‰ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        serializable_args = {}
        for key, value in args_dict.items():
            if isinstance(value, (int, float, str, bool, list, dict, type(None))):
                serializable_args[key] = value
            else:
                serializable_args[key] = str(value)
        
        with open(args_path, 'w') as f:
            json.dump(serializable_args, f, indent=2)
        
        return args_path

# ------------------------------
# å…±äº«å†…å­˜æ•°æ®é›†ç¼“å­˜ç®¡ç†
# ------------------------------
class SharedMemoryDatasetCache:
    """
    ä½¿ç”¨å…±äº«å†…å­˜ç¼“å­˜æ•°æ®é›†ï¼Œæ”¯æŒå¤šè¿›ç¨‹å…±äº«
    é€šè¿‡ MD5 æ ¡éªŒå”¯ä¸€æ ‡è¯†æ•°æ®é›†
    """
    @staticmethod
    def compute_md5(file_path, chunk_size=8192):
        """è®¡ç®—æ–‡ä»¶ MD5"""
        md5 = hashlib.md5()
        with open(file_path, 'rb') as f:
            while chunk := f.read(chunk_size):
                md5.update(chunk)
        return md5.hexdigest()
    
    @staticmethod
    def get_shm_name(h5_path, dataset_name):
        """æ ¹æ®æ–‡ä»¶è·¯å¾„å’Œæ•°æ®é›†åç§°ç”Ÿæˆå…±äº«å†…å­˜åç§°"""
        md5_hash = SharedMemoryDatasetCache.compute_md5(h5_path)
        return f"glint_{md5_hash}_{dataset_name}"
    
    @staticmethod
    def try_attach_or_create(h5_path, dataset_name, data_array=None):
        """
        å°è¯•è¿æ¥ç°æœ‰å…±äº«å†…å­˜ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        
        Args:
            h5_path: HDF5 æ–‡ä»¶è·¯å¾„
            dataset_name: æ•°æ®é›†åç§° (images/heatmaps)
            data_array: å¦‚æœéœ€è¦åˆ›å»ºï¼Œæä¾›çš„æ•°æ®æ•°ç»„
            
        Returns:
            (shm, np_array, is_new) - å…±äº«å†…å­˜å¯¹è±¡ã€numpy æ•°ç»„ã€æ˜¯å¦æ–°åˆ›å»º
        """
        shm_name = SharedMemoryDatasetCache.get_shm_name(h5_path, dataset_name)
        
        # å°è¯•è¿æ¥å·²å­˜åœ¨çš„å…±äº«å†…å­˜
        try:
            shm = shared_memory.SharedMemory(name=shm_name)
            print(f"  âœ“ è¿æ¥åˆ°ç°æœ‰å…±äº«å†…å­˜: {shm_name}")
            
            # éœ€è¦çŸ¥é“å½¢çŠ¶å’Œ dtype æ‰èƒ½åˆ›å»º numpy æ•°ç»„
            # æˆ‘ä»¬å°†å½¢çŠ¶å’Œ dtype ä¿¡æ¯å­˜å‚¨åœ¨å…±äº«å†…å­˜çš„å‰å‡ ä¸ªå­—èŠ‚
            meta_size = 64  # é¢„ç•™ 64 å­—èŠ‚å­˜å‚¨å…ƒæ•°æ®
            meta_bytes = bytes(shm.buf[:meta_size])
            
            # è§£æå…ƒæ•°æ®: ndim(4) + shape(8*ndim) + dtype_len(4) + dtype_str
            ndim = int.from_bytes(meta_bytes[0:4], 'little')
            shape = tuple(int.from_bytes(meta_bytes[4+i*8:4+(i+1)*8], 'little') 
                         for i in range(ndim))
            dtype_len = int.from_bytes(meta_bytes[4+ndim*8:8+ndim*8], 'little')
            dtype_str = meta_bytes[8+ndim*8:8+ndim*8+dtype_len].decode('utf-8')
            
            # åˆ›å»º numpy æ•°ç»„è§†å›¾
            np_array = np.ndarray(shape, dtype=np.dtype(dtype_str), 
                                 buffer=shm.buf[meta_size:])
            
            return shm, np_array, False
            
        except FileNotFoundError:
            # å…±äº«å†…å­˜ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
            if data_array is None:
                raise ValueError("éœ€è¦æä¾› data_array æ¥åˆ›å»ºæ–°çš„å…±äº«å†…å­˜")
            
            print(f"  âœ“ åˆ›å»ºæ–°çš„å…±äº«å†…å­˜: {shm_name}")
            
            # å‡†å¤‡å…ƒæ•°æ®
            shape = data_array.shape
            dtype_str = str(data_array.dtype)
            ndim = len(shape)
            
            meta_size = 64
            meta_bytes = bytearray(meta_size)
            meta_bytes[0:4] = ndim.to_bytes(4, 'little')
            for i, s in enumerate(shape):
                meta_bytes[4+i*8:4+(i+1)*8] = s.to_bytes(8, 'little')
            dtype_bytes = dtype_str.encode('utf-8')
            meta_bytes[4+ndim*8:8+ndim*8] = len(dtype_bytes).to_bytes(4, 'little')
            meta_bytes[8+ndim*8:8+ndim*8+len(dtype_bytes)] = dtype_bytes
            
            # åˆ›å»ºå…±äº«å†…å­˜
            total_size = meta_size + data_array.nbytes
            shm = shared_memory.SharedMemory(name=shm_name, create=True, size=total_size)
            
            # å†™å…¥å…ƒæ•°æ®
            shm.buf[:meta_size] = meta_bytes
            
            # åˆ›å»º numpy æ•°ç»„è§†å›¾å¹¶å¤åˆ¶æ•°æ®
            np_array = np.ndarray(shape, dtype=data_array.dtype, 
                                 buffer=shm.buf[meta_size:])
            np_array[:] = data_array[:]
            
            print(f"    å½¢çŠ¶: {shape}, dtype: {dtype_str}, å¤§å°: {total_size / 1024**2:.1f} MB")
            
            return shm, np_array, True
    
    @staticmethod
    def cleanup(h5_path, dataset_names=['images', 'heatmaps']):
        """æ¸…ç†å…±äº«å†…å­˜ï¼ˆåœ¨ç¨‹åºç»“æŸæ—¶è°ƒç”¨ï¼‰"""
        for dataset_name in dataset_names:
            shm_name = SharedMemoryDatasetCache.get_shm_name(h5_path, dataset_name)
            try:
                shm = shared_memory.SharedMemory(name=shm_name)
                shm.close()
                shm.unlink()
                print(f"  âœ“ æ¸…ç†å…±äº«å†…å­˜: {shm_name}")
            except FileNotFoundError:
                pass

# ------------------------------
# Dataset
# ------------------------------
class GlintH5(Dataset):
    def __init__(self, path, preload=True, use_shared_memory=False):
        """
        Args:
            path: HDF5 æ–‡ä»¶è·¯å¾„
            preload: æ˜¯å¦é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
            use_shared_memory: æ˜¯å¦ä½¿ç”¨å…±äº«å†…å­˜ï¼ˆæ”¯æŒå¤šè¿›ç¨‹å…±äº«æ•°æ®ï¼‰
        """
        self.path = path
        self.shm_imgs = None
        self.shm_hms = None
        
        f = h5py.File(path, "r")
        
        if preload and use_shared_memory:
            # ä½¿ç”¨å…±äº«å†…å­˜åŠ è½½æ•°æ®
            print(f"ä½¿ç”¨å…±äº«å†…å­˜åŠ è½½æ•°æ®é›†: {path}")
            print(f"  MD5: {SharedMemoryDatasetCache.compute_md5(path)[:16]}...")
            
            # å°è¯•è¿æ¥æˆ–åˆ›å»º images å…±äº«å†…å­˜
            imgs_data = f["images"][:]
            self.shm_imgs, self.imgs, is_new_imgs = \
                SharedMemoryDatasetCache.try_attach_or_create(path, "images", imgs_data)
            
            # å°è¯•è¿æ¥æˆ–åˆ›å»º heatmaps å…±äº«å†…å­˜
            hms_data = f["heatmaps"][:].astype("float32")
            self.shm_hms, self.hms, is_new_hms = \
                SharedMemoryDatasetCache.try_attach_or_create(path, "heatmaps", hms_data)
            
            f.close()
            
            if is_new_imgs or is_new_hms:
                print(f"  âœ“ æ•°æ®å·²åŠ è½½åˆ°å…±äº«å†…å­˜ï¼Œå…¶ä»–è¿›ç¨‹å¯ç›´æ¥å¤ç”¨")
            else:
                print(f"  âœ“ å·²å¤ç”¨ç°æœ‰å…±äº«å†…å­˜ä¸­çš„æ•°æ®")
                
        elif preload:
            # æ™®é€šå†…å­˜åŠ è½½ï¼ˆä¸å…±äº«ï¼‰
            print(f"é¢„åŠ è½½æ•°æ®é›†åˆ°å†…å­˜: {path}")
            self.imgs = f["images"][:]
            self.hms = f["heatmaps"][:].astype("float32")
            f.close()
            print(f"  å›¾åƒæ•°æ®: {self.imgs.shape}, {self.imgs.dtype}")
            print(f"  çƒ­åŠ›å›¾æ•°æ®: {self.hms.shape}, {self.hms.dtype}")
        else:
            # ä¿æŒæ–‡ä»¶æ‰“å¼€ï¼ŒæŒ‰éœ€è¯»å–ï¼ˆé€‚ç”¨äºå¤§æ•°æ®é›†ï¼‰
            self.f = f
            self.imgs = f["images"]
            self.hms = f["heatmaps"]
    
    def __len__(self): 
        return self.imgs.shape[0]
        
    def __getitem__(self, i):
        img = torch.from_numpy(self.imgs[i]).float().unsqueeze(0) / 255.0
        hm = torch.from_numpy(self.hms[i] if isinstance(self.hms[i], np.ndarray) else self.hms[i][:].astype("float32"))
        return img, hm
    
    def __del__(self):
        """ææ„æ—¶å…³é—­å…±äº«å†…å­˜è¿æ¥ï¼ˆä½†ä¸ unlinkï¼‰"""
        if self.shm_imgs is not None:
            try:
                self.shm_imgs.close()
            except:
                pass
        if self.shm_hms is not None:
            try:
                self.shm_hms.close()
            except:
                pass

# ------------------------------
# Training Loop
# ------------------------------
def train_one_epoch(model, loader, optimizer, loss_fn, device, logger=None):
    model.train()
    total_loss = 0.0
    n_batches = len(loader)

    for imgs, targets in tqdm(loader, desc="Train", ncols=90, disable=logger is not None):
        imgs, targets = imgs.to(device), targets.to(device)

        optimizer.zero_grad()
        out = model(imgs)
        loss = loss_fn(out, targets)      # â† è¿”å›æ ‡é‡å¼ é‡

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / n_batches

def validate(model, loader, loss_fn, device, logger=None):
    model.eval()
    total_loss = 0.0
    n_batches = len(loader)

    with torch.no_grad():
        for imgs, targets in tqdm(loader, desc="Val", ncols=90, disable=logger is not None):
            imgs, targets = imgs.to(device), targets.to(device)
            out = model(imgs)
            loss = loss_fn(out, targets)
            total_loss += loss.item()

    return total_loss / n_batches

# ------------------------------
# Main
# ------------------------------
def main():
    ap = argparse.ArgumentParser()
    # æ•°æ®å’Œè¾“å‡ºç›¸å…³
    ap.add_argument("--h5", required=True, help="HDF5 æ•°æ®é›†è·¯å¾„")
    ap.add_argument("--output_dir", default=None, help="è¾“å‡ºç›®å½•ï¼ˆä¿å­˜ checkpointsã€æ—¥å¿—ç­‰ï¼‰")
    ap.add_argument("--resume", default=None, help="æ¢å¤è®­ç»ƒçš„ checkpoint è·¯å¾„")
    
    # è®­ç»ƒå‚æ•°
    ap.add_argument("--epochs", type=int, default=50)
    ap.add_argument("--batch", type=int, default=8)
    ap.add_argument("--lr", type=float, default=1e-3)
    ap.add_argument("--val_split", type=float, default=0.1)
    ap.add_argument("--gpu", type=str, default=None, help="æŒ‡å®š GPU è®¾å¤‡ (å¦‚ '0', '1', '0,1' ç­‰)ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨é€‰æ‹©")
    
    # æ•°æ®åŠ è½½ä¼˜åŒ–
    ap.add_argument("--num_workers", type=int, default=4, help="DataLoader å·¥ä½œè¿›ç¨‹æ•°ï¼ˆæ¨è 4-8ï¼‰")
    ap.add_argument("--preload", action="store_true", default=True, help="é¢„åŠ è½½æ•°æ®åˆ°å†…å­˜")
    ap.add_argument("--no_preload", action="store_false", dest="preload", help="ä¸é¢„åŠ è½½æ•°æ®ï¼ˆå¤§æ•°æ®é›†ä½¿ç”¨ï¼‰")
    ap.add_argument("--shared_memory", action="store_true", help="ä½¿ç”¨å…±äº«å†…å­˜ï¼ˆå¤šè¿›ç¨‹è®­ç»ƒå¯å¤ç”¨æ•°æ®ï¼‰")
    ap.add_argument("--cleanup_shm", action="store_true", help="æ¸…ç†å…±äº«å†…å­˜åé€€å‡º")
    
    # Checkpoint ç›¸å…³
    ap.add_argument("--save_freq", type=int, default=10, help="æ¯ N ä¸ª epoch ä¿å­˜ä¸€æ¬¡ checkpoint")
    ap.add_argument("--keep_last_n", type=int, default=3, help="ä¿ç•™æœ€è¿‘ N ä¸ª checkpoint")
    ap.add_argument("--no_save_optimizer", action="store_true", help="ä¸ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå‡å°æ–‡ä»¶å¤§å°ï¼‰")
    
    # å…¼å®¹æ—§å‚æ•°
    ap.add_argument("--save", default="checkpoints", help="[å·²å¼ƒç”¨] ä½¿ç”¨ --output_dir æ›¿ä»£")
    ap.add_argument("--model_path", default="best.pt", help="[å·²å¼ƒç”¨] ä½¿ç”¨ --output_dir æ›¿ä»£")
    ap.add_argument("--loss", default="focal", choices=["focal","bce","dice","hybrid"])
    ap.add_argument("--alpha", type=float, default=1.0)
    ap.add_argument("--gamma", type=float, default=2.0)
    ap.add_argument("--lam_focal", type=float, default=1.0)
    ap.add_argument("--lam_bce",   type=float, default=0)
    ap.add_argument("--lam_dice",  type=float, default=0)
    ap.add_argument("--div_weight", type=float, default=0.05, help="ç›¸ä¼¼åº¦æƒ©ç½šç³»æ•°")
    ap.add_argument("--div_mode", default="cosine", choices=["overlap", "cosine", "kl"], help="ç›¸ä¼¼åº¦æƒ©ç½šæ¨¡å¼")
    ap.add_argument("--lam_agg", type=float, default=0.2, help="èšåˆç±»æ— å…³é¡¹æ€»æƒé‡")
    ap.add_argument("--agg_mode", default="max", choices=["max","sum"], help="èšåˆæ–¹å¼ï¼šmaxæˆ–sum-clip")
    ap.add_argument("--agg_wF", type=float, default=1.0, help="èšåˆé¡¹å†…éƒ¨ Focal å æ¯”")
    ap.add_argument("--agg_wB", type=float, default=0, help="èšåˆé¡¹å†…éƒ¨ BCE   å æ¯”")
    ap.add_argument("--agg_wD", type=float, default=0, help="èšåˆé¡¹å†…éƒ¨ Dice  å æ¯”")
    args = ap.parse_args()
    
    # å¦‚æœåªæ˜¯æ¸…ç†å…±äº«å†…å­˜ï¼Œæ‰§è¡Œæ¸…ç†åé€€å‡º
    if args.cleanup_shm:
        print(f"æ¸…ç†å…±äº«å†…å­˜: {args.h5}")
        SharedMemoryDatasetCache.cleanup(args.h5)
        print("å®Œæˆï¼")
        return
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    if args.output_dir is None:
        # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºç›®å½•åï¼ˆåŸºäºæ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        args.output_dir = f"runs/train_{timestamp}"
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–æ—¥å¿—å’Œ checkpoint ç®¡ç†å™¨
    logger = TrainingLogger(args.output_dir)
    checkpoint_mgr = CheckpointManager(args.output_dir, save_optimizer=not args.no_save_optimizer)
    
    logger.log("=" * 80)
    logger.log("å¼€å§‹è®­ç»ƒ")
    logger.log("=" * 80)
    logger.log(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    logger.log(f"æ•°æ®é›†: {args.h5}")
    
    # ä¿å­˜è®­ç»ƒå‚æ•°
    checkpoint_mgr.save_training_args(args)
    logger.log(f"è®­ç»ƒå‚æ•°å·²ä¿å­˜åˆ°: {os.path.join(args.output_dir, 'training_args.json')}")

    # åˆ›å»ºæ•°æ®é›†ï¼ˆæ”¯æŒé¢„åŠ è½½å’Œå…±äº«å†…å­˜ï¼‰
    logger.log("\nåŠ è½½æ•°æ®é›†...")
    dataset = GlintH5(args.h5, preload=args.preload, use_shared_memory=args.shared_memory)
    n_total = len(dataset)
    n_val = int(n_total * args.val_split)
    n_train = n_total - n_val
    train_set, val_set = random_split(dataset, [n_train, n_val])
    
    # åˆ›å»º DataLoaderï¼ˆå¯ç”¨å¤šè¿›ç¨‹å’Œ pin_memory ä¼˜åŒ–ï¼‰
    train_loader = DataLoader(
        train_set, 
        batch_size=args.batch, 
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=True,
        persistent_workers=True if args.num_workers > 0 else False
    )
    val_loader = DataLoader(
        val_set, 
        batch_size=args.batch,
        num_workers=max(1, args.num_workers // 2),  # éªŒè¯æ—¶ç”¨è¾ƒå°‘çš„ worker
        pin_memory=True,
        persistent_workers=True if args.num_workers > 0 else False
    )

    logger.log(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {n_train}, éªŒè¯é›†æ ·æœ¬æ•°: {n_val}")
    logger.log(f"Batch size: {args.batch}, Workers: {args.num_workers}")

    # è®¾ç½® GPU è®¾å¤‡
    if args.gpu is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu
        logger.log(f"æŒ‡å®š GPU è®¾å¤‡: {args.gpu}")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.log(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if device == "cuda":
        logger.log(f"GPU è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        logger.log(f"å½“å‰ GPU: {torch.cuda.get_device_name(0)}")
    
    # åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = UNet(in_ch=1, out_ch=8).to(device)
    loss_fn = build_loss(
        args.loss,
        alpha=args.alpha, gamma=args.gamma,
        lam_focal=args.lam_focal, lam_bce=args.lam_bce, lam_dice=args.lam_dice,
        div_weight=args.div_weight, div_mode=args.div_mode,
        lam_agg=args.lam_agg, agg_mode=args.agg_mode,
        agg_weights=(args.agg_wF, args.agg_wB, args.agg_wD)
    )
    
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    
    logger.log(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    logger.log(f"Loss å‡½æ•°: {args.loss}")
    logger.log(f"ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€: {'å¦' if args.no_save_optimizer else 'æ˜¯'}")
    
    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
    start_epoch = 1
    best_loss = float("inf")
    
    if args.resume:
        logger.log(f"\næ¢å¤è®­ç»ƒ: {args.resume}")
        try:
            checkpoint_info = checkpoint_mgr.load_checkpoint(args.resume, model, optimizer, device)
            start_epoch = checkpoint_info['epoch'] + 1
            best_loss = checkpoint_info['metrics'].get('val', {}).get('total', float("inf"))
            logger.log(f"  ä» Epoch {checkpoint_info['epoch']} æ¢å¤")
            logger.log(f"  æœ€ä½³éªŒè¯ Loss: {best_loss:.4f}")
            logger.log(f"  Checkpoint æ—¶é—´: {checkpoint_info['timestamp']}")
        except Exception as e:
            logger.log(f"  âš ï¸  æ¢å¤å¤±è´¥: {e}")
            logger.log("  å°†ä»å¤´å¼€å§‹è®­ç»ƒ")
            start_epoch = 1
            best_loss = float("inf")
    
    logger.log(f"\nå¼€å§‹è®­ç»ƒ: Epoch {start_epoch} -> {args.epochs}")
    logger.log("=" * 80 + "\n")
    
    # è®­ç»ƒå¾ªç¯
        # è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, args.epochs + 1):
        epoch_start_time = time.time()
        
        # è®­ç»ƒå’ŒéªŒè¯
        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device, logger)
        val_loss = validate(model, val_loader, loss_fn, device, logger)
        
        epoch_time = time.time() - epoch_start_time
        current_lr = optimizer.param_groups[0]['lr']
        
        # åˆ›å»ºå…¼å®¹çš„æŒ‡æ ‡å­—å…¸ï¼ˆä¸ºäº†æ—¥å¿—è®°å½•ï¼‰
        tr_metrics = {"total": train_loss, "focal": train_loss, "bce": 0.0, "dice": 0.0, "div": 0.0}
        va_metrics = {"total": val_loss, "focal": val_loss, "bce": 0.0, "dice": 0.0, "div": 0.0}
        
        # è®°å½•æ—¥å¿—
        logger.log_epoch(epoch, tr_metrics, va_metrics, current_lr, epoch_time)
        
        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        is_best = val_loss < best_loss
        if is_best:
            best_loss = val_loss
            checkpoint_mgr.save_checkpoint(
                epoch, model, optimizer, args,
                {'train': tr_metrics, 'val': va_metrics},
                is_best=True
            )
            logger.log(f"  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Loss: {best_loss:.4f})")
        
        # å®šæœŸä¿å­˜ checkpoint
        if epoch % args.save_freq == 0:
            checkpoint_mgr.save_checkpoint(
                epoch, model, optimizer, args,
                {'train': tr_metrics, 'val': va_metrics},
                filename=f"checkpoint_epoch_{epoch:03d}.pth"
            )
            logger.log(f"  ğŸ’¾ ä¿å­˜ checkpoint (Epoch {epoch})")
        
        # æ€»æ˜¯ä¿å­˜æœ€æ–°çš„ checkpoint
        checkpoint_mgr.save_latest(epoch, model, optimizer, args, {'train': tr_metrics, 'val': va_metrics})
    
    logger.log("\n" + "=" * 80)
    logger.log("è®­ç»ƒå®Œæˆï¼")
    logger.log(f"æœ€ä½³éªŒè¯ Loss: {best_loss:.4f}")
    logger.log(f"æ¨¡å‹ä¿å­˜ä½ç½®: {args.output_dir}")
    logger.log("=" * 80)

if __name__ == "__main__":
    main()
