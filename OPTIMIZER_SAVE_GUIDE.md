# ä¼˜åŒ–å™¨çŠ¶æ€ä¿å­˜é€‰é¡¹è¯´æ˜

## âœ¨ æ–°åŠŸèƒ½

æ·»åŠ äº† `--no_save_optimizer` å‚æ•°ï¼Œå…è®¸æ§åˆ¶æ˜¯å¦åœ¨ checkpoint ä¸­ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ã€‚

**é»˜è®¤è¡Œä¸º**ï¼šä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆæ¨èï¼Œæ”¯æŒå®Œç¾æ¢å¤è®­ç»ƒï¼‰

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: éœ€è¦æ¢å¤è®­ç»ƒï¼ˆé»˜è®¤è¡Œä¸ºï¼‰

```bash
# ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆé»˜è®¤è¡Œä¸ºï¼Œæ— éœ€é¢å¤–å‚æ•°ï¼‰
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir runs/exp1 \
    --epochs 100
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¯ä»¥å®Œç¾æ¢å¤è®­ç»ƒï¼ˆåŒ…æ‹¬ momentumã€å­¦ä¹ ç‡è°ƒåº¦ç­‰ï¼‰
- âœ… é€‚åˆé•¿æ—¶é—´è®­ç»ƒã€å¯èƒ½ä¸­æ–­çš„åœºæ™¯

**ç¼ºç‚¹**ï¼š
- âŒ æ–‡ä»¶è¾ƒå¤§ï¼ˆçº¦ 2 å€å¤§å°ï¼‰

### åœºæ™¯ 2: åªéœ€è¦æ¨¡å‹æƒé‡

```bash
# ä¸ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir runs/exp2 \
    --no_save_optimizer \
    --epochs 100
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ–‡ä»¶å¤§å°å‡å°‘çº¦ 50%
- âœ… èŠ‚çœç£ç›˜ç©ºé—´
- âœ… é€‚åˆåªéœ€è¦æ¨ç†çš„æ¨¡å‹

**ç¼ºç‚¹**ï¼š
- âŒ æ— æ³•å®Œç¾æ¢å¤è®­ç»ƒçŠ¶æ€
- âŒ ä½¿ç”¨ `--resume` æ—¶ä¼šä»å¤´åˆå§‹åŒ–ä¼˜åŒ–å™¨

## ğŸ“Š æ–‡ä»¶å¤§å°å¯¹æ¯”

ä»¥ UNet æ¨¡å‹ä¸ºä¾‹ï¼š

| é…ç½® | Checkpoint å¤§å° | è¯´æ˜ |
|-----|----------------|------|
| é»˜è®¤ï¼ˆä¿å­˜ä¼˜åŒ–å™¨ï¼‰ | ~200 MB | æ¨¡å‹æƒé‡ (~100MB) + ä¼˜åŒ–å™¨çŠ¶æ€ (~100MB) |
| `--no_save_optimizer` | ~100 MB | ä»…æ¨¡å‹æƒé‡ (~100MB) |

**èŠ‚çœç©ºé—´**ï¼šå¯¹äº 300 ä¸ª epochï¼Œsave_freq=50 çš„è®­ç»ƒï¼š
- é»˜è®¤ï¼š6 ä¸ª checkpoint Ã— 200MB = 1.2 GB
- ä¸ä¿å­˜ä¼˜åŒ–å™¨ï¼š6 ä¸ª checkpoint Ã— 100MB = 600 MB
- **èŠ‚çœï¼š600 MB (50%)**

## ğŸ”„ æ¢å¤è®­ç»ƒçš„å½±å“

### ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆæ¨èï¼‰

```bash
# è®­ç»ƒï¼ˆé»˜è®¤ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰
python train_glint_unet.py --h5 train_256.h5 --output_dir exp --epochs 100

# æ¢å¤è®­ç»ƒï¼ˆå®Œç¾ç»§ç»­ï¼‰
python train_glint_unet.py --h5 train_256.h5 --output_dir exp \
    --resume exp/latest_checkpoint.pth --epochs 200
```

**æ¢å¤å†…å®¹**ï¼š
- âœ… æ¨¡å‹æƒé‡
- âœ… ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆmomentum ç¼“å†²åŒºï¼‰
- âœ… å­¦ä¹ ç‡
- âœ… Epoch è®¡æ•°
- âœ… è®­ç»ƒæŒ‡æ ‡

### ä¸ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€

```bash
# è®­ç»ƒ
python train_glint_unet.py --h5 train_256.h5 --output_dir exp \
    --no_save_optimizer --epochs 100

# æ¢å¤è®­ç»ƒï¼ˆéƒ¨åˆ†æ¢å¤ï¼‰
python train_glint_unet.py --h5 train_256.h5 --output_dir exp \
    --resume exp/latest_checkpoint.pth --epochs 200
```

**æ¢å¤å†…å®¹**ï¼š
- âœ… æ¨¡å‹æƒé‡
- âŒ ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆä¼šé‡æ–°åˆå§‹åŒ–ï¼‰
- âš ï¸  å­¦ä¹ ç‡ï¼ˆä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„å€¼ï¼‰
- âœ… Epoch è®¡æ•°
- âœ… è®­ç»ƒæŒ‡æ ‡

**å½±å“**ï¼š
- ä¼˜åŒ–å™¨çš„ momentum ç¼“å†²åŒºä¸¢å¤±ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š
- éœ€è¦æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡ä»¥é€‚åº”æ¢å¤è®­ç»ƒ

## ğŸ’¡ æ¨èä½¿ç”¨æ–¹å¼

### æƒ…å†µ A: æ­£å¼è®­ç»ƒï¼ˆæ¨èä¿å­˜ä¼˜åŒ–å™¨ï¼‰

```bash
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/final_model \
    --epochs 300 \
    --save_freq 10
```

**åŸå› **ï¼š
- è®­ç»ƒæ—¶é—´é•¿ï¼Œå¯èƒ½ä¸­æ–­
- éœ€è¦å®Œç¾æ¢å¤è®­ç»ƒçŠ¶æ€
- ç£ç›˜ç©ºé—´å……è¶³

### æƒ…å†µ B: å¿«é€Ÿå®éªŒï¼ˆå¯ä¸ä¿å­˜ä¼˜åŒ–å™¨ï¼‰

```bash
python train_glint_unet.py \
    --h5 train_256.h5 \
    --output_dir experiments/quick_test \
    --no_save_optimizer \
    --epochs 50
```

**åŸå› **ï¼š
- è®­ç»ƒæ—¶é—´çŸ­ï¼Œä¸å¤ªå¯èƒ½ä¸­æ–­
- ä¸»è¦å…³æ³¨æœ€ç»ˆæ¨¡å‹æƒé‡
- èŠ‚çœç£ç›˜ç©ºé—´

### æƒ…å†µ C: æ‰¹é‡å®éªŒï¼ˆå¯ä¸ä¿å­˜ä¼˜åŒ–å™¨ï¼‰

```bash
for loss in focal bce hybrid; do
    python train_glint_unet.py \
        --h5 train_256.h5 \
        --output_dir exp_${loss} \
        --no_save_optimizer \
        --loss ${loss} \
        --epochs 50 &
done
wait
```

**åŸå› **ï¼š
- å¤šä¸ªå®éªŒå¹¶è¡Œï¼Œç£ç›˜å‹åŠ›å¤§
- ä¸»è¦å¯¹æ¯”æœ€ç»ˆæ€§èƒ½ï¼Œä¸éœ€è¦æ¢å¤
- èŠ‚çœå¤§é‡ç£ç›˜ç©ºé—´

## ğŸ” æ£€æŸ¥ Checkpoint å†…å®¹

```python
import torch

# åŠ è½½ checkpoint
checkpoint = torch.load('runs/exp1/best_model.pth')

# æŸ¥çœ‹åŒ…å«çš„é”®
print("Checkpoint åŒ…å«çš„é”®:", checkpoint.keys())
# è¾“å‡º: dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'train_args', 'metrics', 'timestamp'])

# æ£€æŸ¥æ˜¯å¦åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€
if 'optimizer_state_dict' in checkpoint:
    print("âœ… åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€")
else:
    print("âŒ ä¸åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€")
```

## ğŸ“ æŠ€æœ¯å®ç°

### CheckpointManager ç±»

```python
class CheckpointManager:
    def __init__(self, output_dir, save_optimizer=True):
        self.save_optimizer = save_optimizer
        # ...
    
    def save_checkpoint(self, epoch, model, optimizer, ...):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            # åªæœ‰åœ¨ save_optimizer=True æ—¶æ‰ä¿å­˜
        }
        
        if self.save_optimizer and optimizer is not None:
            checkpoint['optimizer_state_dict'] = optimizer.state_dict()
        
        torch.save(checkpoint, path)
```

### åŠ è½½æ—¶çš„å…¼å®¹æ€§

```python
def load_checkpoint(self, checkpoint_path, model, optimizer=None, device='cuda'):
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # å¦‚æœ checkpoint ä¸­æœ‰ä¼˜åŒ–å™¨çŠ¶æ€ä¸”ä¼ å…¥äº† optimizerï¼Œåˆ™åŠ è½½
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```

## âš™ï¸ é»˜è®¤è¡Œä¸º

- **é»˜è®¤å€¼**ï¼šä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆæ— éœ€ä»»ä½•å‚æ•°ï¼‰
- **åŸå› **ï¼šä¿è¯è®­ç»ƒå¯æ¢å¤æ€§ï¼Œè¿™æ˜¯æœ€å®‰å…¨çš„é€‰æ‹©
- **ç¦ç”¨**ï¼šä½¿ç”¨ `--no_save_optimizer` æ˜ç¡®ç¦ç”¨ä»¥å‡å°æ–‡ä»¶å¤§å°

## âœ… æ€»ç»“

| ç‰¹æ€§ | é»˜è®¤ï¼ˆä¿å­˜ä¼˜åŒ–å™¨ï¼‰ | `--no_save_optimizer` |
|------|-----------------|---------------------|
| æ–‡ä»¶å¤§å° | å¤§ (~200MB) | å° (~100MB) |
| æ¢å¤è®­ç»ƒ | å®Œç¾æ¢å¤ | éƒ¨åˆ†æ¢å¤ |
| ç£ç›˜å ç”¨ | é«˜ | ä½ |
| æ¨èåœºæ™¯ | æ­£å¼è®­ç»ƒã€é•¿æ—¶é—´è®­ç»ƒ | å¿«é€Ÿå®éªŒã€åªéœ€æ¨ç† |

**å»ºè®®**ï¼š
- ğŸ¯ å¦‚æœä¸ç¡®å®šï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®ï¼ˆä¿å­˜ä¼˜åŒ–å™¨ï¼Œæ— éœ€é¢å¤–å‚æ•°ï¼‰
- ğŸ’¾ å¦‚æœç£ç›˜ç©ºé—´ç´§å¼ ï¼Œæ·»åŠ  `--no_save_optimizer`
- ğŸ”„ å¦‚æœéœ€è¦æ¢å¤è®­ç»ƒï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®ï¼ˆä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰
